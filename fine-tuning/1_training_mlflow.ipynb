{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Open Source LLM using the Azure ML Python SDK (MLflow) \n",
    "\n",
    "### Overview\n",
    "Azure ML Workspace is compatible with MLflow and can be used as an MLflow Tracking Server, as described in the following official guide from Microsoft. MLflow provides features such as experiment tracking, model management, and model deployment, allowing you to manage data science and machine learning workflows more efficiently and systematically. Below are the main advantages of using Azure ML and MLflow together.\n",
    "\n",
    "#### 1. Experiment tracking and management\n",
    "You can systematically manage the parameters, metrics, and artifacts of all your experiments. Integrating with Azur eML allows you to easily track and manage this information within your Azure ML workspace.\n",
    "\n",
    "#### 2. Model management\n",
    "MLflow provides a model registry for model versioning. Integrate with AzureML to systematically manage and deploy all versions of your models. When combined with AzureML's deployment capabilities, models can be easily deployed to a variety of environments (e.g. Azure Kubernetes Service, Azure Container Instances).\n",
    "\n",
    "#### 3. Reproducibility and collaboration\n",
    "MLflow records the parameters and environment of every experiment, so you can accurately reproduce the experiment. This is very useful when you need to redo the same experiment across collaborating team members, or when you need to rerun an experiment at a later date.\n",
    "\n",
    "#### 4. CI/CD integration\n",
    "MLflow makes it easy to implement continuous integration (CI) and continuous deployment (CD) of machine learning models. Integrate with Azure DevOps or GitHub Actions to automatically run training, validation, and deployment processes as model changes occur.\n",
    "\n",
    "When training a model with Hugging Face's Trainer API, if you specify `report_to=\"azure_ml\"`, basic indicators will be automatically logged without any additional code. Of course, you can freely log custom indicators using Bring Your Own Script like the conventional method, but Azure ML's basic logging function is also excellent, so try using it as a baseline.\n",
    "\n",
    "[Note] Please use `Python 3.10 - SDK v2 (azureml_py310_sdkv2)` conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config file\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "with open('config.yml') as f:\n",
    "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
    "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
    "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
    "AZURE_DATA_NAME = d['config']['AZURE_DATA_NAME']    \n",
    "DATA_DIR = d['config']['DATA_DIR']\n",
    "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
    "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
    "IS_DEBUG = d['config']['IS_DEBUG']\n",
    "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
    "\n",
    "azure_env_name = d['train']['azure_env_name']  \n",
    "azure_compute_cluster_name = d['train']['azure_compute_cluster_name']\n",
    "azure_compute_cluster_size = d['train']['azure_compute_cluster_size']\n",
    "\n",
    "!rm -rf $DATA_DIR \n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(CLOUD_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1. Dataset preparation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:2%]\")\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "if IS_DEBUG:\n",
    "    dataset = dataset.select(range(1000))\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset['train']\n",
    "train_dataset.to_json(f\"{DATA_DIR}/train.jsonl\")\n",
    "test_dataset = dataset['test']\n",
    "test_dataset.to_json(f\"{DATA_DIR}/eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2. Training preparation\n",
    "---\n",
    "\n",
    "### 2.1. Configure workspace details\n",
    "\n",
    "To connect to a workspace, we need identifying parameters - a subscription, a resource group, and a workspace name. We will use these details in the MLClient from azure.ai.ml to get a handle on the Azure Machine Learning workspace we need. We will use the default Azure authentication for this hands-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import time\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "from azure.ai.ml import MLClient, Input\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml import command\n",
    "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
    "from azure.ai.ml.entities import Model\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import Output\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = None\n",
    "try:\n",
    "    ml_client = MLClient.from_config(credential)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Create AzureML environment and data\n",
    "\n",
    "Azure ML defines containers (called environment asset) in which your code will run. We can use the built-in environment or build a custom environment (Docker container, conda). \n",
    "This hands-on uses conda yaml. \n",
    "\n",
    "Training data can be used as a dataset stored in the local development environment, but can also be registered as AzureML data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {CLOUD_DIR}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "    - bitsandbytes==0.43.1\n",
    "    - transformers~=4.41\n",
    "    - peft~=0.11\n",
    "    - accelerate~=0.30\n",
    "    - trl==0.9.4\n",
    "    - einops==0.8.0\n",
    "    - datasets==2.20.0\n",
    "    - wandb==0.17.2\n",
    "    - mlflow==2.14.1\n",
    "    - azureml-mlflow==1.56.0\n",
    "    - azureml-sdk==1.56.0\n",
    "    - torchvision==0.18.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_environment_asset(ml_client, env_name, conda_yml=\"cloud/conda.yml\", update=False):\n",
    "    \n",
    "    try:\n",
    "        latest_env_version = max([int(e.version) for e in ml_client.environments.list(name=env_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Environment asset, but will update the Environment.')\n",
    "        else:\n",
    "            env_asset = ml_client.environments.get(name=env_name, version=latest_env_version)\n",
    "            print(f\"Found Environment asset: {env_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        print(f\"Exception: {e}\")        \n",
    "        env_docker_image = Environment(\n",
    "            image=\"mcr.microsoft.com/azureml/curated/acft-hf-nlp-gpu:latest\",\n",
    "            conda_file=conda_yml,\n",
    "            name=env_name,\n",
    "            description=\"Environment created for llm fine-tuning.\",\n",
    "        )\n",
    "        env_asset = ml_client.environments.create_or_update(env_docker_image)\n",
    "        print(f\"Created Environment asset: {env_name}\")\n",
    "        \n",
    "    return env_asset\n",
    "\n",
    "\n",
    "def get_or_create_data_asset(ml_client, data_name, data_local_dir, update=False):\n",
    "    \n",
    "    try:\n",
    "        latest_data_version = max([int(d.version) for d in ml_client.data.list(name=data_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Data asset, but will update the Data.')            \n",
    "        else:\n",
    "            data_asset = ml_client.data.get(name=data_name, version=latest_data_version)\n",
    "            print(f\"Found Data asset: {data_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        data = Data(\n",
    "            path=data_local_dir,\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=f\"{data_name} for fine tuning\",\n",
    "            tags={\"FineTuningType\": \"Instruction\", \"Language\": \"En\"},\n",
    "            name=data_name\n",
    "        )\n",
    "        data_asset = ml_client.data.create_or_update(data)\n",
    "        print(f\"Created Data asset: {data_name}\")\n",
    "        \n",
    "    return data_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_or_create_environment_asset(ml_client, azure_env_name, conda_yml=f\"{CLOUD_DIR}/conda.yml\", update=False)\n",
    "data = get_or_create_data_asset(ml_client, AZURE_DATA_NAME, data_local_dir=DATA_DIR, update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src_train/train_mlflow.py\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec\n",
    "\n",
    "import os\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import argparse\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from datetime import datetime\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_model(args, model_name_or_path=\"microsoft/Phi-3-mini-4k-instruct\"):\n",
    "\n",
    "    model_kwargs = dict(\n",
    "        use_cache=False,\n",
    "        trust_remote_code=True,\n",
    "        #attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=None\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, **model_kwargs)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    tokenizer.model_max_length = args.max_seq_length\n",
    "    tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    return model, tokenizer\n",
    "\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = example[\"messages\"]\n",
    "    # Add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "def main(args):\n",
    "    \n",
    "    ###################\n",
    "    # Hyper-parameters\n",
    "    ###################\n",
    "    # Only overwrite environ if wandb param passed\n",
    "    if len(args.wandb_project) > 0:\n",
    "        os.environ['WANDB_API_KEY'] = args.wandb_api_key    \n",
    "        os.environ[\"WANDB_PROJECT\"] = args.wandb_project\n",
    "    if len(args.wandb_watch) > 0:\n",
    "        os.environ[\"WANDB_WATCH\"] = args.wandb_watch\n",
    "    if len(args.wandb_log_model) > 0:\n",
    "        os.environ[\"WANDB_LOG_MODEL\"] = args.wandb_log_model\n",
    "\n",
    "    use_wandb = len(args.wandb_project) > 0 or (\"WANDB_PROJECT\" in os.environ and len(os.environ[\"WANDB_PROJECT\"]) > 0) \n",
    "\n",
    "    training_config = {\n",
    "        \"bf16\": True,\n",
    "        \"do_eval\": False,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"log_level\": \"info\",\n",
    "        \"logging_steps\": args.logging_steps,\n",
    "        \"logging_strategy\": \"steps\",\n",
    "        \"lr_scheduler_type\": args.lr_scheduler_type,\n",
    "        \"num_train_epochs\": args.epochs,\n",
    "        \"max_steps\": -1,\n",
    "        \"output_dir\": args.output_dir,\n",
    "        \"overwrite_output_dir\": True,\n",
    "        \"per_device_train_batch_size\": args.train_batch_size,\n",
    "        \"per_device_eval_batch_size\": args.eval_batch_size,\n",
    "        \"remove_unused_columns\": True,\n",
    "        \"save_steps\": args.save_steps,\n",
    "        \"save_total_limit\": 1,\n",
    "        \"seed\": args.seed,\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"gradient_checkpointing_kwargs\": {\"use_reentrant\": False},\n",
    "        \"gradient_accumulation_steps\": args.grad_accum_steps,\n",
    "        \"warmup_ratio\": args.warmup_ratio,\n",
    "    }\n",
    "\n",
    "    peft_config = {\n",
    "        \"r\": args.lora_r,\n",
    "        \"lora_alpha\": args.lora_alpha,\n",
    "        \"lora_dropout\": args.lora_dropout,\n",
    "        \"bias\": \"none\",\n",
    "        \"task_type\": \"CAUSAL_LM\",\n",
    "        #\"target_modules\": \"all-linear\",\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        \"modules_to_save\": None,\n",
    "    }\n",
    "\n",
    "    checkpoint_dir = os.path.join(args.output_dir, \"checkpoints\")\n",
    "\n",
    "    train_conf = TrainingArguments(\n",
    "        **training_config,\n",
    "        report_to=\"wandb\" if use_wandb else \"azure_ml\",\n",
    "        run_name=args.wandb_run_name if use_wandb else None,    \n",
    "    )\n",
    "    peft_conf = LoraConfig(**peft_config)\n",
    "    model, tokenizer = load_model(args)\n",
    "\n",
    "    ###############\n",
    "    # Setup logging\n",
    "    ###############\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    )\n",
    "    log_level = train_conf.get_process_log_level()\n",
    "    logger.setLevel(log_level)\n",
    "    datasets.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.set_verbosity(log_level)\n",
    "    transformers.utils.logging.enable_default_handler()\n",
    "    transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "    # Log on each process a small summary\n",
    "    logger.warning(\n",
    "        f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "        + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    "    )\n",
    "    logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "    logger.info(f\"PEFT parameters {peft_conf}\")    \n",
    "\n",
    "    ##################\n",
    "    # Data Processing\n",
    "    ##################\n",
    "    train_dataset = load_dataset('json', data_files=os.path.join(args.train_dir, 'train.jsonl'), split='train')\n",
    "    eval_dataset = load_dataset('json', data_files=os.path.join(args.train_dir, 'eval.jsonl'), split='train')\n",
    "    column_names = list(train_dataset.features)\n",
    "\n",
    "    processed_train_dataset = train_dataset.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=10,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Applying chat template to train_sft\",\n",
    "    )\n",
    "\n",
    "    processed_eval_dataset = eval_dataset.map(\n",
    "        apply_chat_template,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        num_proc=10,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Applying chat template to test_sft\",\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run() as run:        \n",
    "        ###########\n",
    "        # Training\n",
    "        ###########\n",
    "        trainer = SFTTrainer(\n",
    "            model=model,\n",
    "            args=train_conf,\n",
    "            peft_config=peft_conf,\n",
    "            train_dataset=processed_train_dataset,\n",
    "            eval_dataset=processed_eval_dataset,\n",
    "            max_seq_length=args.max_seq_length,\n",
    "            dataset_text_field=\"text\",\n",
    "            tokenizer=tokenizer,\n",
    "            packing=True,\n",
    "        )\n",
    "\n",
    "        # Show current memory stats\n",
    "        gpu_stats = torch.cuda.get_device_properties(0)\n",
    "        start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "        logger.info(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "        logger.info(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "        \n",
    "        last_checkpoint = None\n",
    "        if os.path.isdir(checkpoint_dir):\n",
    "            checkpoints = [os.path.join(checkpoint_dir, d) for d in os.listdir(checkpoint_dir)]\n",
    "            if len(checkpoints) > 0:\n",
    "                checkpoints.sort(key=os.path.getmtime, reverse=True)\n",
    "                last_checkpoint = checkpoints[0]        \n",
    "\n",
    "        trainer_stats = trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "        #############\n",
    "        # Logging\n",
    "        #############\n",
    "        metrics = trainer_stats.metrics\n",
    "\n",
    "        # Show final memory and time stats \n",
    "        used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "        used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "        used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "        lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "\n",
    "        logger.info(f\"{metrics['train_runtime']} seconds used for training.\")\n",
    "        logger.info(f\"{round(metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "        logger.info(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "        logger.info(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "        logger.info(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "        logger.info(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
    "                \n",
    "        trainer.log_metrics(\"train\", metrics)\n",
    "\n",
    "        model_info = mlflow.transformers.log_model(\n",
    "            transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer},\n",
    "            #prompt_template=prompt_template,\n",
    "            #signature=signature,\n",
    "            artifact_path=args.model_dir,  # This is a relative path to save model files within MLflow run\n",
    "        )\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    # setup argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # curr_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "\n",
    "    # hyperparameters\n",
    "    parser.add_argument(\"--train_dir\", default=\"data\", type=str, help=\"Input directory for training\")\n",
    "    parser.add_argument(\"--model_dir\", default=\"./model\", type=str, help=\"output directory for model\")\n",
    "    parser.add_argument(\"--epochs\", default=1, type=int, help=\"number of epochs\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"./output_dir\", type=str, help=\"directory to temporarily store when training a model\")\n",
    "    parser.add_argument(\"--train_batch_size\", default=8, type=int, help=\"training - mini batch size for each gpu/process\")\n",
    "    parser.add_argument(\"--eval_batch_size\", default=8, type=int, help=\"evaluation - mini batch size for each gpu/process\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-06, type=float, help=\"learning rate\")\n",
    "    parser.add_argument(\"--logging_steps\", default=2, type=int, help=\"logging steps\")\n",
    "    parser.add_argument(\"--save_steps\", default=100, type=int, help=\"save steps\")    \n",
    "    parser.add_argument(\"--grad_accum_steps\", default=4, type=int, help=\"gradient accumulation steps\")\n",
    "    parser.add_argument(\"--lr_scheduler_type\", default=\"linear\", type=str)\n",
    "    parser.add_argument(\"--seed\", default=0, type=int, help=\"seed\")\n",
    "    parser.add_argument(\"--warmup_ratio\", default=0.2, type=float, help=\"warmup ratio\")\n",
    "    parser.add_argument(\"--max_seq_length\", default=2048, type=int, help=\"max seq length\")\n",
    "    parser.add_argument(\"--save_merged_model\", type=bool, default=False)\n",
    "\n",
    "    # lora hyperparameters\n",
    "    parser.add_argument(\"--lora_r\", default=16, type=int, help=\"lora r\")\n",
    "    parser.add_argument(\"--lora_alpha\", default=16, type=int, help=\"lora alpha\")\n",
    "    parser.add_argument(\"--lora_dropout\", default=0.05, type=float, help=\"lora dropout\")\n",
    "    \n",
    "    # wandb params\n",
    "    parser.add_argument(\"--wandb_api_key\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_run_name\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--wandb_watch\", type=str, default=\"gradients\") # options: false | gradients | all\n",
    "    parser.add_argument(\"--wandb_log_model\", type=str, default=\"false\") # options: false | true\n",
    "\n",
    "    # parse args\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # return args\n",
    "    return args\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #sys.argv = ['']\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3. Training\n",
    "---\n",
    "\n",
    "### 3.1. Create the compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "### Create the compute cluster\n",
    "try:\n",
    "    compute = ml_client.compute.get(azure_compute_cluster_name)\n",
    "    print(\"The compute cluster already exists! Reusing it for the current run\")\n",
    "except Exception as ex:\n",
    "    print(\n",
    "        f\"Looks like the compute cluster doesn't exist. Creating a new one with compute size {azure_compute_cluster_size}!\"\n",
    "    )\n",
    "    try:\n",
    "        print(\"Attempt #1 - Trying to create a dedicated compute\")\n",
    "        tier = 'LowPriority' if USE_LOWPRIORITY_VM else 'Dedicated'\n",
    "        compute = AmlCompute(\n",
    "            name=azure_compute_cluster_name,\n",
    "            size=azure_compute_cluster_size,\n",
    "            tier=tier,\n",
    "            max_instances=1,  # For multi node training set this to an integer value more than 1\n",
    "        )\n",
    "        ml_client.compute.begin_create_or_update(compute).wait()\n",
    "    except Exception as e:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Start training job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml.entities import ResourceConfiguration\n",
    "\n",
    "job = command(\n",
    "    inputs=dict(\n",
    "        #train_dir=Input(type=\"uri_folder\", path=DATA_DIR), # Get data from local path\n",
    "        train_dir=Input(path=f\"{AZURE_DATA_NAME}@latest\"),  # Get data from Data asset\n",
    "        epoch=d['train']['epoch'],\n",
    "        train_batch_size=d['train']['train_batch_size'],\n",
    "        eval_batch_size=d['train']['eval_batch_size'],  \n",
    "        model_dir=d['train']['model_dir']\n",
    "    ),\n",
    "    code=\"./src_train\",  # local path where the code is stored\n",
    "    compute=azure_compute_cluster_name,\n",
    "    command=\"python train_mlflow.py --train_dir ${{inputs.train_dir}} --epochs ${{inputs.epoch}} --train_batch_size ${{inputs.train_batch_size}} --eval_batch_size ${{inputs.eval_batch_size}} --model_dir ${{inputs.model_dir}}\",\n",
    "    #environment=\"azureml://registries/azureml/environments/acft-hf-nlp-gpu/versions/57\", # Use built-in Environment asset\n",
    "    environment=f\"{azure_env_name}@latest\",\n",
    "    distribution={\n",
    "        \"type\": \"PyTorch\",\n",
    "        \"process_count_per_instance\": 1, # For multi-gpu training set this to an integer value more than 1\n",
    "    },\n",
    ")\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "ml_client.jobs.stream(returned_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(returned_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the `trained_model` output is available\n",
    "job_name = returned_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4. (Optional) Create model asset and get fine-tuned LLM to local folder\n",
    "---\n",
    "\n",
    "### 3.1. Create model asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_model_asset(ml_client, model_name, job_name, model_dir=\"outputs\", model_type=\"custom_model\", update=False):\n",
    "    \n",
    "    try:\n",
    "        latest_model_version = max([int(m.version) for m in ml_client.models.list(name=model_name)])\n",
    "        if update:\n",
    "            raise ResourceExistsError('Found Model asset, but will update the Model.')\n",
    "        else:\n",
    "            model_asset = ml_client.models.get(name=model_name, version=latest_model_version)\n",
    "            print(f\"Found Model asset: {model_name}. Will not create again\")\n",
    "    except (ResourceNotFoundError, ResourceExistsError) as e:\n",
    "        print(f\"Exception: {e}\")        \n",
    "        model_path = f\"azureml://jobs/{job_name}/outputs/artifacts/paths/{model_dir}/\"    \n",
    "        run_model = Model(\n",
    "            name=model_name,        \n",
    "            path=model_path,\n",
    "            description=\"Model created from run.\",\n",
    "            type=model_type # mlflow_model, custom_model, triton_model\n",
    "        )\n",
    "        model_asset = ml_client.models.create_or_update(run_model)\n",
    "        print(f\"Created Model asset: {model_name}\")\n",
    "\n",
    "    return model_asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_model_name = d['serve']['azure_model_name']\n",
    "model_dir = d['train']['model_dir']\n",
    "model = get_or_create_model_asset(ml_client, azure_model_name, job_name, model_dir, model_type=\"custom_model\", update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Get fine-tuned LLM to local folder\n",
    "You can copy it to your local directory to perform inference or serve the model in Azure environment. (e.g., real-time endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model (this is optional)\n",
    "local_model_dir = \"./artifact_downloads\"\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "ml_client.models.download(name=azure_model_name, download_path=local_model_dir, version=model.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
